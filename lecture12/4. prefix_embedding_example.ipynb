{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prefix Tuning 과정 실습\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 파이토치 / 트랜스포머 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 토크나이저 로드\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 문장과 라벨 정의\n",
    "input_texts = [\"The movie was great!\", \"The movie was terrible.\", \"I loved the acting.\", \"The plot was boring.\"]\n",
    "labels = [1, 0, 1, 0]  # 예시로 긍정(1), 부정(0) 라벨 사용\n",
    "\n",
    "# 입력 텍스트를 토큰화하고 텐서로 변환\n",
    "inputs = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True, max_length=10)\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기본 BERT 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.6899866461753845\n",
      "Epoch 2/3, Loss: 0.6152526140213013\n",
      "Epoch 3/3, Loss: 0.5844820737838745\n"
     ]
    }
   ],
   "source": [
    "basic_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(basic_model.parameters(), lr=1e-5)\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    basic_model.train()  # 모델을 학습 모드로 설정\n",
    "    \n",
    "    optimizer.zero_grad()  # 옵티마이저의 기울기 초기화\n",
    "    \n",
    "    # 모델의 출력 계산\n",
    "    outputs = basic_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # 손실 계산\n",
    "    loss = criterion(logits, labels)\n",
    "    \n",
    "    # 역전파를 통해 기울기 계산\n",
    "    loss.backward()\n",
    "    \n",
    "    # 옵티마이저를 통해 파라미터 업데이트\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prefix-튜닝 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixTuningModel(nn.Module):\n",
    "    def __init__(self, model_name, prefix_length):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.prefix_length = prefix_length  # prefix_length를 클래스 속성으로 저장\n",
    "        self.prefix_embeddings = nn.Parameter(torch.randn((self.model.config.num_hidden_layers, prefix_length, self.model.config.hidden_size)))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        batch_size = input_ids.size(0)\n",
    "        # (num_hidden_layers, 1, prefix_length, hidden_size) 형태로 변환\n",
    "        # (num_hidden_layers, batch_size, prefix_length, hidden_size)로 확장\n",
    "        prefix_embeds = self.prefix_embeddings.unsqueeze(1).expand(-1, batch_size, -1, -1)\n",
    "        \n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = list(outputs.hidden_states)\n",
    "\n",
    "        ########## 각 은닉 상태에 프리픽스 임베딩 결합 #############\n",
    "        for i in range(1, len(hidden_states)):  # hidden_states[0]은 입력 임베딩임\n",
    "            hidden_states[i] = torch.cat((prefix_embeds[i-1], hidden_states[i]), dim=1)\n",
    "            \n",
    "        # 확장된 attention_mask 생성\n",
    "        extended_attention_mask = torch.cat([\n",
    "            torch.ones((batch_size, self.prefix_length), dtype=attention_mask.dtype, device=attention_mask.device),\n",
    "            attention_mask\n",
    "        ], dim=1)\n",
    "        \n",
    "        # 결합된 은닉 상태를 사용하여 최종 출력 계산\n",
    "        inputs_embeds = hidden_states[-1]\n",
    "        return self.model(inputs_embeds=inputs_embeds, attention_mask=extended_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.7236154675483704\n",
      "Epoch 2/3, Loss: 0.6990016102790833\n",
      "Epoch 3/3, Loss: 0.7475633025169373\n"
     ]
    }
   ],
   "source": [
    "# 손실 함수와 옵티마이저 정의\n",
    "prefix_model = PrefixTuningModel(model_name, prefix_length=5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(prefix_model.parameters(), lr=1e-5)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(num_epochs):\n",
    "    prefix_model.train()  # 모델을 학습 모드로 설정\n",
    "    \n",
    "    optimizer.zero_grad()  # 옵티마이저의 기울기 초기화\n",
    "    \n",
    "    # 모델의 출력 계산\n",
    "    outputs = prefix_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # 손실 계산\n",
    "    loss = criterion(logits, labels)\n",
    "    \n",
    "    # 역전파를 통해 기울기 계산\n",
    "    loss.backward()\n",
    "    \n",
    "    # 옵티마이저를 통해 파라미터 업데이트\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1996, 3185, 2001, 2307,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 비교 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Model Accuracy: 1.0\n",
      "Prefix Tuning Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 평가 함수 정의\n",
    "def evaluate_model(model, input_ids, attention_mask, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        accuracy = (predicted_labels == labels).float().mean().item()\n",
    "        return accuracy\n",
    "\n",
    "# 평가 데이터 준비\n",
    "eval_input_texts = [\"The movie was fantastic!\", \"The plot was dull.\"]\n",
    "eval_labels = torch.tensor([1, 0])\n",
    "eval_inputs = tokenizer(eval_input_texts, return_tensors='pt', padding=True, truncation=True, max_length=10)\n",
    "eval_input_ids = eval_inputs['input_ids']\n",
    "eval_attention_mask = eval_inputs['attention_mask']\n",
    "\n",
    "# 기본 모델 평가\n",
    "basic_model_accuracy = evaluate_model(basic_model, eval_input_ids, eval_attention_mask, eval_labels)\n",
    "print(f\"Basic Model Accuracy: {basic_model_accuracy}\")\n",
    "\n",
    "# Prefix Tuning 모델 평가\n",
    "prefix_model_accuracy = evaluate_model(prefix_model, eval_input_ids, eval_attention_mask, eval_labels)\n",
    "print(f\"Prefix Tuning Model Accuracy: {prefix_model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP에서 추가로 알아둬야 할 중간변수들\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 은닉층 및 텐서 차원 관련\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_hidden_layers: 12\n",
      "hidden_size: 768\n",
      "prefix_length: 5\n"
     ]
    }
   ],
   "source": [
    "print('num_hidden_layers:', model.model.config.num_hidden_layers)\n",
    "# 트랜스포머 모델의 인코더(또는 디코더) 계층의 수\n",
    "# 트랜스포머 모델은 여러 개의 인코더 및 디코더 계층으로 구성\n",
    "# 각 계층은 셀프 어텐션과 피드포워드 신경망으로 구성됨\n",
    "# 계층이 많을수록 모델은 더 복잡한 패턴을 학습할수 있음.\n",
    "# 여기서는 12개의 인코더 계층이 있다는 것을 의미\n",
    "print('hidden_size:', model.model.config.hidden_size)\n",
    "# 각 계층의 은닉 상태 벡터의 차원. BERT-base 모델에서 hidden_size=768은 각 토큰이 768차원 벡터로 표현된다는 것을 의미\n",
    "print('prefix_length:', model.prefix_embeddings.size(1))\n",
    "# prefix_length는 5로 설정했으므로, 5개의 토큰으로 이루어진 prefix를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 5, 768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3-D tensor를 생성 ( 12 x 5 x 768 )\n",
    "model.prefix_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_ids\n",
    "___\n",
    "- 입력 텍스트를 토큰화한 후, 각 토큰을 고유한 정수 ID로 변환한 것. \n",
    "- 토크나이저는 텍스트를 사전에 정의된 어휘(vocabulary)에 따라 토큰으로 분할하고, 각 토큰을 어휘에서의 인덱스로 매핑\n",
    "\n",
    "예를 들어, BERT 모델의 토크나이저를 사용하면 다음과 같은 과정이 진행됨.\n",
    "\n",
    "1. 텍스트: \"The movie was great!\"\n",
    "2. 토큰화: `[\"[CLS]\", \"the\", \"movie\", \"was\", \"great\", \"!\", \"[SEP]\"]`\n",
    "3. 정수 ID 변환: `[101, 1996, 3185, 2001, 2307, 999, 102]`\n",
    "\n",
    " `101`은 `[CLS]` 토큰의 ID이고, `102`는 `[SEP]` 토큰의 ID\n",
    "`[CLS]` 토큰은 문장의 시작을 나타내며, `[SEP]` 토큰은 문장의 끝을 나타냄."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention_mask\n",
    "___\n",
    "- 입력 시퀀스에서 실제 단어와 패딩 토큰을 구분하는 데 사용됨.\n",
    "- 패딩 토큰은 입력 시퀀스 길이를 일정하게 맞추기 위해 추가된 토큰. \n",
    "- `attention_mask`는 각 토큰 위치가 실제 단어인지 패딩인지 나타내는 바이너리 값의 시퀀스임.\n",
    "\n",
    "예를 들어, 입력 시퀀스의 최대 길이가 10이고, 실제 입력이 더 짧은 경우 패딩을 추가.\n",
    "\n",
    "1. 실제 입력: `[101, 1996, 3185, 2001, 2307, 999, 102]`\n",
    "2. 패딩 추가 후: `[101, 1996, 3185, 2001, 2307, 999, 102, 0, 0, 0]`\n",
    "3. `attention_mask`: `[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]`\n",
    "\n",
    "여기서 `1`은 실제 단어 위치를 나타내고, `0`은 패딩 위치를 나타냄."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
