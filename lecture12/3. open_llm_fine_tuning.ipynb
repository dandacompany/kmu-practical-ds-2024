{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì˜¤í”ˆ ì–¸ì–´ëª¨ë¸ Fine-Tuning ì´ë¡ \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶œì²˜ : https://www.catalyzex.com\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/38258a93151d57a073fe5cfccefd443863942478/2-Figure1-1.png\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fine-Tuning ì˜ ê°œë…\n",
    "\n",
    "ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ íŠ¹ì • ë„ë©”ì¸ì´ë‚˜ ì„œë¹„ìŠ¤ ìƒí™©ì— ë§ê²Œ ìµœì í™”í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì— ëŒ€í•œ íŠ¹ì„±ì„ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ í†µí•´ ì¶”ì¶œí•˜ê³ , ëª¨ë¸ì˜ ì¶œë ¥ ì¸µì„ ìƒˆë¡œìš´ ì‘ì—…ì— ë§ê²Œ ì¡°ì •í•œ í›„, ì „ì²´ ëª¨ë¸ì„ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì— ë§ê²Œ ì¬í•™ìŠµí•˜ëŠ” ê³¼ì •\n",
    "\n",
    "**ì ˆì°¨ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ â†’ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì˜ íŠ¹ì„± ì¶”ì¶œ â†’ ìƒˆë¡œìš´ ë¶„ë¥˜ê¸° ì¶”ê°€ â†’ ë¯¸ì„¸ì¡°ì • ë¥¼ ê±°ì¹¨**\n",
    "\n",
    "### 2. Fine-Tuning ì´ í•„ìš”í•œ ìƒí™©\n",
    "\n",
    "1. ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ : íŠ¹ì • ë„ë©”ì¸ì´ë‚˜ ì„œë¹„ìŠ¤ ìƒí™©ì— ìµœì í™” í•´ì•¼ í• ë•Œ\n",
    "2. ë¶€ì¡±í•œ ë¼ë²¨ ë°ì´í„° ë³´ì™„ :  ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ìœ ìš©í•œ í•˜ìœ„ê³„ì¸µ ë°ì´í„°ë¥¼ í™œìš©\n",
    "\n",
    "### 3. Fune-Tuningì˜ ì–´ë ¤ìš´ì \n",
    "\n",
    "1. **ë„ë©”ì¸ ì°¨ì´(Domain Shift)**:\n",
    "    - **ë¬¸ì œ**: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì´ í•™ìŠµí•œ ë°ì´í„°ì™€ ìƒˆë¡œìš´ ë°ì´í„° ê°„ì— ì°¨ì´ê°€ í´ ê²½ìš°, ëª¨ë¸ì´ ê¸°ëŒ€í•œ ëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ.\n",
    "    - **ì„¤ëª…**: ì˜ˆë¥¼ ë“¤ì–´, ìì—° ì´ë¯¸ì§€ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ ì˜ë£Œ ì´ë¯¸ì§€ì— ì ìš©í•˜ë ¤ í•  ë•Œ, ë°ì´í„° íŠ¹ì„±ì´ í¬ê²Œ ë‹¤ë¥´ë©´ ì„±ëŠ¥ì´ ì €í•˜ë¨.\n",
    "2. **ê³¼ì í•©(Overfitting)**:\n",
    "    - **ë¬¸ì œ**: ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì´ ì‘ì„ ê²½ìš°, ë¯¸ì„¸ ì¡°ì • ê³¼ì •ì—ì„œ ëª¨ë¸ì´ ê³¼ì í•©ë  ìœ„í—˜ì´ ë†’ì•„ì§\n",
    "    - **ì„¤ëª…**: ëª¨ë¸ì´ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì— ë„ˆë¬´ ë§ì¶”ì–´ì ¸, ì¼ë°˜í™” ëŠ¥ë ¥ì´ ë–¨ì–´ì§.\n",
    "3. **í•™ìŠµë¥  ì¡°ì ˆ**:\n",
    "    - **ë¬¸ì œ**: í•™ìŠµë¥ ì„ ì˜ëª» ì„¤ì •í•˜ë©´, ê¸°ì¡´ì˜ í•™ìŠµëœ ê°€ì¤‘ì¹˜ê°€ í¬ê²Œ ë³€í•˜ê±°ë‚˜, í•™ìŠµì´ ì œëŒ€ë¡œ ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ.\n",
    "    - **ì„¤ëª…**: ë„ˆë¬´ ë†’ì€ í•™ìŠµë¥ ì€ ëª¨ë¸ì„ ë¶ˆì•ˆì •í•˜ê²Œ ë§Œë“¤ê³ , ë„ˆë¬´ ë‚®ì€ í•™ìŠµë¥ ì€ í•™ìŠµ ì†ë„ë¥¼ ëŠë¦¬ê²Œ ë§Œë“¦\n",
    "4. **ì»´í“¨íŒ… ìì›ê³¼ ì‹œê°„**:\n",
    "    - **ë¬¸ì œ**: í° ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°ëŠ” ë§ì€ ê³„ì‚° ìì›ê³¼ ì‹œê°„ì´ í•„ìš”í•¨.\n",
    "    - **ì„¤ëª…**: íŠ¹íˆ ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ê²½ìš°, í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­ì´ ë§¤ìš° ë†’ì•„ ê°œì¸/ì¤‘ì†Œ ê·œëª¨ì—ì„œ ìˆ˜í–‰í•˜ê¸° ì–´ë ¤ì›Œì§.\n",
    "\n",
    "### 4. PEFT(Parameter Efiicient Fine Tuning) ì˜ ì¥ì \n",
    "\n",
    "1. **íš¨ìœ¨ì ì¸ í•™ìŠµ**\n",
    "    - PEFTëŠ” ëª¨ë¸ì˜ ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ ì¡°ì •í•˜ê¸° ë•Œë¬¸ì—, ì „ì²´ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ íš¨ìœ¨ì \n",
    "    - ë©”ëª¨ë¦¬ ì‚¬ìš©ê³¼ ê³„ì‚° ìì›ì„ ì ˆì•½í•  ìˆ˜ ìˆì–´, ë” ì ì€ ìì›ìœ¼ë¡œë„ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„± ê°€ëŠ¥\n",
    "2. **ì €ë¹„ìš© ê³ íš¨ìœ¨**\n",
    "    - ì ì€ í•™ìŠµ ë°ì´í„°ì™€ ìì›ìœ¼ë¡œë„ ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì • ê°€ëŠ¥\n",
    "    - ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ê³¼ ê°™ì´ í° ëª¨ë¸ì„ ì‚¬ìš©í•  ë•Œ ìœ ë¦¬\n",
    "3. **ë¹ ë¥¸ í•™ìŠµ**\n",
    "    - íŠ¹ì • íŒŒë¼ë¯¸í„°ë§Œ ì¡°ì •í•˜ë¯€ë¡œ, í•™ìŠµ ì†ë„ê°€ ë¹ ë¦„.\n",
    "    - ì‹¤ì‹œê°„ ì‘ìš©ì—ë„ ì í•©\n",
    "4. **ì ì‘ë ¥**\n",
    "    - ëª¨ë¸ì˜ íŠ¹ì • ë¶€ë¶„ì„ ì¡°ì •í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ ì œê³µë˜ê³  ìˆì–´, ë‹¤ì–‘í•œ ë„ë©”ì¸ê³¼ ì‘ì—…ì— Adapting í• ìˆ˜ ìˆìŒ.\n",
    "    - Adapters, Low-Rank Adaptation (LoRA), Prefix-Tuning ë“±ì˜ ê¸°ë²•ì„ í†µí•´, í•„ìš”í•œ ë¶€ë¶„ë§Œ ì¡°ì • ê°€ëŠ¥.\n",
    "5. **ì¼ê´€ëœ ì„±ëŠ¥ê³¼ ì•ˆì •ì„±**\n",
    "    - PEFTëŠ” ëª¨ë¸ì˜ ì£¼ìš” íŠ¹ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ íŠ¹ì • ì‘ì—…ì— ë§ëŠ” ì¡°ì •ì„ í•  ìˆ˜ ìˆì–´, ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ì œê³µ\n",
    "    - ê³¼ì í•©ì˜ ìœ„í—˜ì„ ì¤„ì´ê³ , ì¼ë°˜í™” ëŠ¥ë ¥ì„ ìœ ì§€í•˜ê²Œ ë¨.\n",
    "\n",
    "### 5. LLM ë¶„ì•¼ì˜ ëŒ€í‘œì  PEFT ë°©ë²•ë¡ \n",
    "\n",
    "- ì°¸ê³  ë¬¸ì„œ\n",
    "    - https://medium.com/@abonia/llm-series-parameter-efficient-fine-tuning-e9839fae44ac\n",
    "    - https://lightning.ai/pages/community/article/lora-llm/\n",
    "    - https://huggingface.co/docs/transformers/peft\n",
    "    - https://github.com/Lightning-AI/lit-llama/ (fine-tune êµ¬í˜„ ì½”ë“œ)\n",
    "1. **LoRA(Low Rank Adaptation)**\n",
    "    \n",
    "    [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
    "    \n",
    "    !https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/lora-5-1024x511.png\n",
    "    \n",
    "    1. LoRAëŠ” Low-Rank Factorizaton ë°©ë²•ì„ í™œìš©í•´ LLMì˜ Linear Layerì— ëŒ€í•œ ì—…ë°ì´íŠ¸ë¥¼ ê·¼ì‚¬í™”í•˜ëŠ” ê¸°ìˆ .\n",
    "    2. ê¸°ë³¸ ì›ë¦¬ëŠ” ëª¨ë¸ì˜ ê³ ì°¨ì› íŒŒë¼ë¯¸í„°ë¥¼ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ íˆ¬ì˜í•œë’¤, ì €ì°¨ì› ê³µê°„ì—ì„œì˜ ì¶”ê°€ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµí•˜ê³  ì´ë¥¼ ì›ë˜ ëª¨ë¸ì— ë³‘í•©í•˜ì—¬ ì¶œë ¥ì„ ì¡°ì ˆ.\n",
    "    3. ê³ ì°¨ì› íŒŒë¼ë¯¸í„° ëŒ€ì‹  ì €ì°¨ì› íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ë¯€ë¡œ, í•„ìš”í•œ ë©”ëª¨ë¦¬ì™€ ê³„ì‚° ìì›ì´ í¬ê²Œ ì¤„ì–´ë“¤ì–´,  ëª¨ë¸ì˜ ìµœì¢… ì„±ëŠ¥ì— ê±°ì˜ ì˜í–¥ì„ ì£¼ì§€ ì•Šìœ¼ë©´ì„œ í›ˆë ¨ ì†ë„ë¥¼ ë†’ì„\n",
    "    4. ì•Œê³ ë¦¬ì¦˜ ì„¤ëª… (Transformer ì—ì‹œ)\n",
    "        - ì›ë˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ W: í¬ê¸°ê°€ d x kì¸ í–‰ë ¬\n",
    "        - ì €ì°¨ì› í–‰ë ¬ A:  í¬ê¸°ê°€ d x r ì¸ í–‰ë ¬ë¡œ, ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ íˆ¬ì˜.\n",
    "        - ì €ì°¨ì› í–‰ë ¬ B: í¬ê¸°ê°€ r x k ì¸ í–‰ë ¬ë¡œ, ë‹¤ì‹œ ê³ ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜.\n",
    "        - í•™ìŠµ: í•™ìŠµ ê³¼ì •ì—ì„œ Aì™€ B í–‰ë ¬ì„ í•™ìŠµ. ì´ë•Œ rì€ dë‚˜ kë³´ë‹¤ í›¨ì”¬ ì‘ê¸° ë•Œë¬¸ì—, í•„ìš”í•œ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ í¬ê²Œ ì¤„ì–´ë“¬.\n",
    "        - ì¶œë ¥ ê³„ì‚°: í•™ìŠµëœ ì €ì°¨ì› í–‰ë ¬ì„ ì‚¬ìš©í•˜ì—¬, ì›ë˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ ì¶œë ¥ì„ ì¡°ì •.\n",
    "        \n",
    "        $$\n",
    "         W +\\alpha \\cdot (A \\cdot B)\n",
    "        $$\n",
    "        \n",
    "    5. QLoRA : ê¸°ì¡´ RoLAì—ì„œ ì–‘ìí™” ê¸°ë²•ìœ¼ë¡œ ì‘ìš©í•œ í˜•íƒœë¡œ ê³„ì‚° íš¨ìœ¨ì„±ì„ í•œë‹¨ê³„ ë” ê·¹ëŒ€í™”. (ì €ì°¨ì› í–‰ë ¬ì˜ ê°’ì„ ì •ë°€ë„ê°€ ë‚®ì€ ë°ì´í„° íƒ€ì…(ì˜ˆ: 8-bit ì •ìˆ˜)ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•˜ê³  ì—°ì‚°)\n",
    "    6. ì˜ˆì‹œ ì½”ë“œ\n",
    "        \n",
    "        ```python\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        from transformers import AutoModelForSequenceClassification\n",
    "        \n",
    "        class LoRALayer(nn.Module):\n",
    "            def __init__(self, original_layer, rank):\n",
    "                super().__init__()\n",
    "                self.original_layer = original_layer\n",
    "                self.rank = rank\n",
    "                self.lora_A = nn.Parameter(torch.randn((original_layer.weight.size(0), rank)))\n",
    "                self.lora_B = nn.Parameter(torch.randn((rank, original_layer.weight.size(1))))\n",
    "        \n",
    "            def forward(self, x):\n",
    "                return self.original_layer(x) + (x @ self.lora_B.t() @ self.lora_A.t())\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                setattr(model, name, LoRALayer(module, rank=8))\n",
    "        \n",
    "        # í•™ìŠµ ê³¼ì •ì€ ì¼ë°˜ì ì¸ ë°©ì‹ê³¼ ë™ì¼í•©ë‹ˆë‹¤.\n",
    "        ```\n",
    "        \n",
    "2. **Prompt Tuning**\n",
    "    \n",
    "    [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)\n",
    "    \n",
    "    1. ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³ , ì…ë ¥ ë°ì´í„°ì— ëŒ€í•œ ì„ë² ë”©ì„ ìˆ˜ì •í•˜ì—¬ ëª¨ë¸ì´ ì›í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰\n",
    "    2. êµ¬í˜„ ì ˆì°¨ ì„¤ëª…\n",
    "        - **ê¸°ì¡´ ëª¨ë¸ ì¤€ë¹„**\n",
    "            - ë¯¸ë¦¬ í•™ìŠµëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(ì˜ˆ: GPT, BERT)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "        - **ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìì—°ì–´ê°€ ì•„ë‹Œ íŠ¹ì • ëª©ì ì—  ìµœì í™”ëœ í›ˆë ¨ê°€ëŠ¥í•œ ì„ë² ë”© ë²¡í„°)**\n",
    "            - ëª¨ë¸ì˜ ì…ë ¥ ì„ë² ë”© ì°¨ì›ê³¼ ë™ì¼í•œ ì°¨ì›ì˜ ì„ì˜ì˜ ì´ˆê¸°ê°’ì„ ê°€ì§„ ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ ë²¡í„°ë¥¼ ìƒì„±.\n",
    "        - **ì…ë ¥ ë°ì´í„°ì™€ ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ ê²°í•©**\n",
    "            - ì…ë ¥ ë°ì´í„° ì•ì— ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ ë²¡í„°ë¥¼ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì— ì…ë ¥.\\\n",
    "        - **í•™ìŠµ ê³¼ì •**\n",
    "            - ëª¨ë¸ì„ í•™ìŠµí•˜ë©´ì„œ ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ ë²¡í„°ë¥¼ ìµœì í™”. ì´ë•Œ ëª¨ë¸ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ê³ ì •ëœ ìƒíƒœë¡œ ìœ ì§€.\n",
    "            - ì¼ë°˜ì ì¸ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ ìµœì í™” ë°©ë²•(SGD)ì„ ì‚¬ìš©í•´ ì—­ì „íŒŒ í•™ìŠµ\n",
    "    3. ì˜ˆì‹œ ì½”ë“œ\n",
    "        \n",
    "        ```python\n",
    "        import torch\n",
    "        from torch import nn\n",
    "        from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "        \n",
    "        class PromptTuningModel(nn.Module):\n",
    "            def __init__(self, model_name, prompt_length):\n",
    "                super(PromptTuningModel, self).__init__()\n",
    "                self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "                self.prompt_embeddings = nn.Parameter(torch.randn(prompt_length, self.model.config.hidden_size))\n",
    "        \n",
    "            def forward(self, input_ids, attention_mask):\n",
    "                batch_size = input_ids.size(0)\n",
    "                prompt_embeds = self.prompt_embeddings.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "                \n",
    "                inputs_embeds = self.model.get_input_embeddings()(input_ids)\n",
    "                inputs_embeds = torch.cat((prompt_embeds, inputs_embeds), dim=1)\n",
    "                \n",
    "                extended_attention_mask = torch.cat((torch.ones(batch_size, self.prompt_embeddings.size(0), device=input_ids.device), attention_mask), dim=1)\n",
    "                \n",
    "                outputs = self.model(inputs_embeds=inputs_embeds, attention_mask=extended_attention_mask)\n",
    "                return outputs\n",
    "        \n",
    "        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì¤€ë¹„\n",
    "        model_name = \"bert-base-uncased\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = PromptTuningModel(model_name, prompt_length=5)\n",
    "        \n",
    "        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„\n",
    "        inputs = tokenizer(\"This is a test sentence.\", return_tensors='pt')\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # ì†ì‹¤ ê³„ì‚° ë° ì—­ì „íŒŒ\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # ì˜ˆì‹œ ë ˆì´ë¸”\n",
    "        loss = nn.CrossEntropyLoss()(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # ìµœì í™” ë‹¨ê³„\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "        optimizer.step()\n",
    "        ```\n",
    "        \n",
    "3. **Prefix-Tuning**\n",
    "    \n",
    "    [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)\n",
    "    \n",
    "    1. ê¸°ì¡´ì˜ ë¯¸ì„¸ ì¡°ì • ë°©ë²•ë“¤ê³¼ëŠ” ë‹¬ë¦¬, Prefix-Tuningì€ ëª¨ë¸ì˜ ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•˜ëŠ” ëŒ€ì‹  ì…ë ¥ ë°ì´í„°ì— ì¶”ê°€ì ì¸ ë²¡í„°(í”„ë¦¬í”½ìŠ¤)ë¥¼ ë¶™ì—¬ì„œ í•™ìŠµí•˜ëŠ” ë°©ë²•\n",
    "    2. ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ëŠ” ê³ ì •(frozen)ëœ ìƒíƒœë¡œ ìœ ì§€ì‹œí‚´. ëŒ€ì‹ , í”„ë¦¬í”½ìŠ¤ ë²¡í„°ë§Œì„ ì¶”ê°€ë¡œ í•™ìŠµì‹œí‚´. ì´ë¥¼ í†µí•´ ì „ì²´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¬í•™ìŠµí•˜ëŠ” ë¶€ë‹´ì„ ì¤„ì„\n",
    "    3. ì˜ˆë¥¼ ë“¤ì–´, ì…ë ¥ ë¬¸ì¥ì´ â€œThis movie is greatâ€ë¼ë©´, í”„ë¦¬í”½ìŠ¤ ë²¡í„°ê°€ ì¶”ê°€ëœ ì…ë ¥ì€ â€œ[í”„ë¦¬í”½ìŠ¤ ë²¡í„°] This movie is greatâ€ê°€ ë˜ëŠ” ê²ƒì„.\n",
    "    4. ì˜ˆì‹œ ì½”ë“œ\n",
    "        \n",
    "        ```python\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "        \n",
    "        class PrefixTuningModel(nn.Module):\n",
    "            def __init__(self, model_name, prefix_length):\n",
    "                super().__init__()\n",
    "                self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "                self.prefix_embeddings = nn.Parameter(torch.randn((self.model.config.num_hidden_layers, prefix_length, self.model.config.hidden_size)))\n",
    "        \n",
    "            def forward(self, input_ids, attention_mask):\n",
    "                prefix_embeds = self.prefix_embeddings.unsqueeze(1).expand(-1, input_ids.size(0), -1, -1)\n",
    "                outputs = self.model(input_ids, attention_mask, output_hidden_states=True)\n",
    "                hidden_states = outputs.hidden_states\n",
    "                for i, hidden_state in enumerate(hidden_states):\n",
    "                    hidden_states[i] = torch.cat((prefix_embeds[i], hidden_state), dim=1)\n",
    "                return self.model(inputs_embeds=hidden_states[-1], attention_mask=attention_mask)\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = PrefixTuningModel('bert-base-uncased', prefix_length=5)\n",
    "        \n",
    "        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„\n",
    "        inputs = tokenizer(\"This is a test sentence.\", return_tensors='pt')\n",
    "        outputs = model(**inputs)\n",
    "        ```\n",
    "        \n",
    "\n",
    "[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì˜¤í”ˆ ì–¸ì–´ëª¨ë¸ Fine-Tuning ì‹¤ìŠµ\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ref\n",
    "  * https://huggingface.co/docs/transformers/training\n",
    "\n",
    "  * https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://lightning.ai/ ì—ì„œ ì§„í–‰í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
    "* ë¬´ë£Œ íšŒì›ê°€ì…í›„, A10G ì´ìƒì—ì„œ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "* êµ¬ê¸€ Colab ë„ ê°€ëŠ¥í•˜ì§€ë§Œ, êµ¬ê¸€ë“œë¼ì´ë¸Œì— ë°ì´í„°ì…‹ì„ ì¤€ë¹„í›„ ë§ˆìš´íŠ¸í•´ì•¼ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets bitsandbytes peft trl accelerate --upgrade -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import wandb\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* í›ˆë ¨ ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 577\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob('dataset/consult/*.csv')\n",
    "\n",
    "df = pd.concat([pd.read_csv(file) for file in files], ignore_index=True)\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABí…ŒìŠ¤íŠ¸ë€ ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
       "      <td>ABí…ŒìŠ¤íŠ¸ëŠ” ë‘ ê°€ì§€ ì´ìƒì˜ ë³€í˜•ì„ ë¹„êµí•˜ì—¬ ì–´ë–¤ ë³€í˜•ì´ ë” íš¨ê³¼ì ì¸ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABí…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
       "      <td>ABí…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•  ë•ŒëŠ” ëª©í‘œ ì„¤ì •, ìƒ˜í”Œ í¬ê¸° ê²°ì •, ëœë¤í™”, í†µê³„ì  ê°€ì„¤ ê²€ì •,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABí…ŒìŠ¤íŠ¸ì—ì„œ ìœ ì˜ìˆ˜ì¤€ê³¼ ê²€ì •ë ¥ì˜ ì¤‘ìš”ì„±ì€ ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
       "      <td>ìœ ì˜ìˆ˜ì¤€ì€ ê°€ì„¤ì´ ì°¸ì¼ ë•Œ ì˜¤ë¥˜ë¥¼ ë²”í•  í™•ë¥ ì„ ë‚˜íƒ€ë‚´ë©°, ê²€ì •ë ¥ì€ ëŒ€ë¦½ê°€ì„¤ì´ ì°¸ì¼ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABí…ŒìŠ¤íŠ¸ ê²°ê³¼ í•´ì„ ì‹œ ì–´ë–¤ ì ì„ ì£¼ì˜í•´ì•¼ í•˜ë‚˜ìš”?</td>\n",
       "      <td>ABí…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í•´ì„í•  ë•ŒëŠ” í†µê³„ì  ìœ ì˜ì„± ë¿ë§Œ ì•„ë‹ˆë¼ ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ì„±ë„ í•¨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABí…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ ì–»ì€ ê²°ê³¼ë¥¼ ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆë‚˜ìš”?</td>\n",
       "      <td>ABí…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ ì–»ì€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì œí’ˆ ë˜ëŠ” ì„œë¹„ìŠ¤ì˜ ê°œì„ ì„ ì‹œë„í•  ìˆ˜ ìˆìœ¼ë©°...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          question  \\\n",
       "0                    ABí…ŒìŠ¤íŠ¸ë€ ë¬´ì—‡ì¸ê°€ìš”?   \n",
       "1   ABí…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?   \n",
       "2   ABí…ŒìŠ¤íŠ¸ì—ì„œ ìœ ì˜ìˆ˜ì¤€ê³¼ ê²€ì •ë ¥ì˜ ì¤‘ìš”ì„±ì€ ë¬´ì—‡ì¸ê°€ìš”?   \n",
       "3    ABí…ŒìŠ¤íŠ¸ ê²°ê³¼ í•´ì„ ì‹œ ì–´ë–¤ ì ì„ ì£¼ì˜í•´ì•¼ í•˜ë‚˜ìš”?   \n",
       "4  ABí…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ ì–»ì€ ê²°ê³¼ë¥¼ ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆë‚˜ìš”?   \n",
       "\n",
       "                                              answer  \n",
       "0  ABí…ŒìŠ¤íŠ¸ëŠ” ë‘ ê°€ì§€ ì´ìƒì˜ ë³€í˜•ì„ ë¹„êµí•˜ì—¬ ì–´ë–¤ ë³€í˜•ì´ ë” íš¨ê³¼ì ì¸ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ...  \n",
       "1  ABí…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•  ë•ŒëŠ” ëª©í‘œ ì„¤ì •, ìƒ˜í”Œ í¬ê¸° ê²°ì •, ëœë¤í™”, í†µê³„ì  ê°€ì„¤ ê²€ì •,...  \n",
       "2  ìœ ì˜ìˆ˜ì¤€ì€ ê°€ì„¤ì´ ì°¸ì¼ ë•Œ ì˜¤ë¥˜ë¥¼ ë²”í•  í™•ë¥ ì„ ë‚˜íƒ€ë‚´ë©°, ê²€ì •ë ¥ì€ ëŒ€ë¦½ê°€ì„¤ì´ ì°¸ì¼ ...  \n",
       "3  ABí…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í•´ì„í•  ë•ŒëŠ” í†µê³„ì  ìœ ì˜ì„± ë¿ë§Œ ì•„ë‹ˆë¼ ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ì„±ë„ í•¨...  \n",
       "4  ABí…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ ì–»ì€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì œí’ˆ ë˜ëŠ” ì„œë¹„ìŠ¤ì˜ ê°œì„ ì„ ì‹œë„í•  ìˆ˜ ìˆìœ¼ë©°...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* í”„ë¡¬í”„íŒ… í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLAMA ê³„ì—´ í”„ë¡¬í”„íŒ… í•¨ìˆ˜ (ëª¨ë¸ ê³„ì—´ì— ë”°ë¼ ë‹¬ë¼ì§)\n",
    "# ì´ í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ì˜ˆì œ ë°ì´í„°ì—ì„œ ì‚¬ìš©ìì™€ ì–´ì‹œìŠ¤í„´íŠ¸ ê°„ì˜ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±\n",
    "# ë°ì´í„°ì…‹ í•„ë“œì— ë”°ë¼ í•„ë“œëª…ì€ ë‹¤ë¥´ê²Œ í•´ì•¼í•¨. ì—¬ê¸°ì„œëŠ” question, answer\n",
    "def generate_prompts(example):\n",
    "    prompt_list = []\n",
    "    for i in range(len(example['question'])):\n",
    "        prompt_list.append(\n",
    "f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>ë‹¤ìŒ ê¸€ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:\n",
    "{example['question'][i]}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "{example['answer'][i]}<|eot_id|>\"\"\"\n",
    "        )\n",
    "    return prompt_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì–‘ìí™” ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bitsandbytes\n",
    "# CUDA ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜, íŠ¹íˆ 8ë¹„íŠ¸ ìµœì í™” í”„ë¡œê·¸ë¨, í–‰ë ¬ ê³±ì…ˆ(LLM.int8()) ë° ì–‘ìí™” í•¨ìˆ˜ì— ëŒ€í•œ ê²½ëŸ‰ ë˜í¼.\n",
    "# 4ë¹„íŠ¸ ì •ë°€ë„ë¡œ ì €ì¥ëœ ëª¨ë¸ì„ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ í•¨. \n",
    "# ê°€ì¤‘ì¹˜ë¥¼ 4ë¹„íŠ¸ë¡œ ì €ì¥í•˜ì§€ë§Œ ê³„ì‚°ì€ ì—¬ì „íˆ â€‹â€‹16ë¹„íŠ¸ ë˜ëŠ” 32ë¹„íŠ¸ë¡œ ì´ë£¨ì–´ì§\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RoLA ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA ì„¤ì • : ì–‘ìí™”ëœ ëª¨ë¸ì—ì„œ Adaptorë¥¼ ë¶™ì—¬ì„œ í•™ìŠµí•  íŒŒë¼ë¯¸í„°ë§Œ ë”°ë¡œ êµ¬ì„±í•¨\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRAì˜ ë­í¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©\n",
    "    lora_alpha = 8,  # LoRAì˜ ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°ë¡œ, í•™ìŠµ ì†ë„ì™€ ê´€ë ¨\n",
    "    lora_dropout = 0.05, # ë“œë¡­ì•„ì›ƒ í™•ë¥ ì„ ì„¤ì •í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # LoRAê°€ ì ìš©ë  ëª¨ë¸ì˜ ëª¨ë“ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì •\n",
    "    bias=\"none\",  # \"none\"ì€ í¸í–¥ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒì„ ì˜ë¯¸\n",
    "    task_type=\"CAUSAL_LM\", # LoRAê°€ ì ìš©ë  ì‘ì—… ìœ í˜•ì„ ì§€ì •\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* í† í¬ë‚˜ì´ì € ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# base_modelì—ì„œ ì‚¬ì „ í•™ìŠµëœ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name,  add_special_tokens=True)\n",
    "# íŒ¨ë”© í† í°ì„ eos í† í°ìœ¼ë¡œ ì„¤ì •\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# íŒ¨ë”© ë°©í–¥ì„ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì„¤ì •\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ë² ì´ìŠ¤ ëª¨ë¸ ì¤€ë¹„ ë° PEFT ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ (Llama ê³„ì—´ ëª¨ë¸ì€ AutoModelForCausalLMë¥¼ ì‚¬ìš©í•´ì•¼í•¨)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\", quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    max_seq_length=128,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"outputs\", # ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
    "        num_train_epochs = 1, # í•™ìŠµ ì—í¬í¬ ìˆ˜\n",
    "        # max_steps=300,   # ìµœëŒ€ ìŠ¤í… ìˆ˜\n",
    "        per_device_train_batch_size=4, # GPUë‹¹ 4ê°œ ë°°ì¹˜\n",
    "        gradient_accumulation_steps=4, # gradient ë°˜ì˜ì„ 4ê°œ step ë§ˆë‹¤\n",
    "        optim=\"paged_adamw_8bit\", # ì˜µí‹°ë§ˆì´ì €\n",
    "        warmup_steps=500, # ì›œì—… ìŠ¤í… ìˆ˜\n",
    "        learning_rate=2e-4,  # í•™ìŠµë¥ \n",
    "        fp16=True, # FP16 ì‚¬ìš© ì—¬ë¶€\n",
    "        logging_steps=100, # ë¡œê¹… ìŠ¤í… ìˆ˜\n",
    "        push_to_hub=False, # HuggingFace Hubì— í‘¸ì‹œ ì—¬ë¶€\n",
    "        report_to='none',  # ë¦¬í¬íŠ¸ ëŒ€ìƒ\n",
    "    ),\n",
    "    peft_config=lora_config,   # LoRA ì„¤ì •ê°’\n",
    "    formatting_func=generate_prompts,   # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í•¨ìˆ˜\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 01:44, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=3.1319512261284723, metrics={'train_runtime': 108.4896, 'train_samples_per_second': 5.318, 'train_steps_per_second': 0.332, 'total_flos': 2541164004605952.0, 'train_loss': 3.1319512261284723, 'epoch': 0.993103448275862})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ í•™ìŠµ ì‹œì‘\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ìµœì¢… íŠœë‹ëª¨ë¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LoRA ì–´ëŒ‘í„° ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "lora_model_name = \"Data-Science-Interview-QNA\"\n",
    "\n",
    "# í›ˆë ¨ëœ ëª¨ë¸ì„ ì–´ëŒ‘í„° ëª¨ë¸ë¡œ ì €ì¥\n",
    "trainer.model.save_pretrained(lora_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map='auto')\n",
    "\n",
    "# ì–´ëŒ‘í„° ëª¨ë¸ ë¡œë“œ\n",
    "base_model = PeftModel.from_pretrained(base_model, lora_model_name, device_map='auto')\n",
    "\n",
    "# ëª¨ë¸ ë³‘í•© ë° ì–¸ë¡œë“œ\n",
    "final_model = base_model.merge_and_unload()\n",
    "\n",
    "# ìµœì¢… ëª¨ë¸ ì €ì¥ (ì•ì— DanteKwak ë¶€ë¶„ì€ ì—¬ëŸ¬ë¶„ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ë§ê²Œ ë°”ê¾¸ì…”ì•¼ í•©ë‹ˆë‹¤.)\n",
    "final_model_name = f'DanteKwak/Llama-3-8B-{lora_model_name}'\n",
    "final_model.save_pretrained(final_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* í—ˆê¹… í˜ì´ìŠ¤ì— ë‚´ ëª¨ë¸ ì—…ë¡œë“œí•˜ê¸°\n",
    "  * https://huggingface.co/new ì—ì„œ ë¨¼ì € ëª¨ë¸ê³µê°„ì„ ë§Œë“œì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Hugging Faceì— ì—…ë¡œë“œë¥¼ ìœ„í•œ API ê°ì²´ ìƒì„±\n",
    "api = HfApi()\n",
    "\n",
    "# Hugging Faceì— ëª¨ë¸ ì—…ë¡œë“œ\n",
    "api.upload_folder(\n",
    "    folder_path=final_model_name,\n",
    "    repo_id=final_model_name,\n",
    "    repo_type='model'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ìƒì„±ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "  * ì–‘ìí™” ëœ ìƒíƒœê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì„œë¹™í•˜ê¸°ì—ëŠ” ë§¤ìš° ëŠë¦½ë‹ˆë‹¤.\n",
    "  * AWQë‚˜ vLLM í˜¹ì€ ë‘˜ë‹¤ ì´ìš©í•´ì„œ ì–‘ìí™”ëœ ìƒíƒœë¡œ ì„œë¹™í•˜ë©´ ì†ë„ê°€ ë¹¨ë¼ì§‘ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤”\n",
      "\n",
      "In A/B testing (also known as split testing), the most important thing is to ensure that the test is designed and executed correctly to produce reliable and actionable results. Here are some key factors to focus on:\n",
      "\n",
      "1. **Clear goals and hypotheses**: Define what you want to achieve with the test and what you expect to see. This will help you determine the success metrics and ensure that the test is focused on a specific goal.\n",
      "2. **Well-defined test variables**: Identify the specific elements you want to test (e.g., button color, headline, CTAs) and ensure that they are well-defined and measurable.\n",
      "3. **Control group and treatment group**: Ensure that both groups are equally representative of your target audience and that the test is randomized to minimize bias.\n",
      "4. **Sufficient sample size**: Make sure the test has enough participants to produce statistically significant results. A general rule of thumb is to aim for at least 1,000 participants per group.\n",
      "5. **Duration of the test**: Run the test for a sufficient amount of time to collect meaningful data. A longer test duration can help reduce the impact of random fluctuations.\n",
      "6. **Data collection and analysis**: Use a reliable data collection method (e.g., Google Analytics, A/B testing software) and analyze the results using statistical methods (e.g., t-tests, ANOVA) to determine significance.\n",
      "7. **Confidence intervals and p-values**: Use confidence intervals and p-values to determine the significance of the results and ensure that the test is not due to chance.\n",
      "8. **Test design and execution**: Ensure that the test is designed and executed correctly, including randomization, data collection, and analysis.\n",
      "9. **Interpretation of results**: Carefully interpret the results, considering factors like sample size, test duration, and potential biases.\n",
      "10. **Actionability**: Ensure that the results are actionable and can be used to inform future design decisions or marketing strategies.\n",
      "\n",
      "By focusing on these key factors, you can increase the reliability and effectiveness of your A/B tests and make data-driven decisions to improve your products, services, or marketing campaigns. ğŸ’¡\n"
     ]
    }
   ],
   "source": [
    "question = \"ABtestì—ì„œ ê°€ì¥ ì¤‘ìš”í•œê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "\n",
    "pipe = pipeline(\"text-generation\", \n",
    "                model=final_model, \n",
    "                tokenizer=tokenizer, \n",
    "                max_new_tokens=512)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"{question}\",}\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    # add_special_tokens=True,\n",
    "    eos_token_id = [ # eos_token_idë¥¼ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„± í† í° ë°˜ë³µ\n",
    "        pipe.tokenizer.eos_token_id,\n",
    "        pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(outputs[0]['generated_text'][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Llama.cpp ì–‘ìí™” ( GGUF íŒŒì¼ ë§Œë“¤ê¸° )\n",
    "  \n",
    "  * ref\n",
    "    * https://github.com/ggerganov/llama.cpp\n",
    "  \n",
    "    * https://github.com/ggerganov/llama.cpp/issues/6819\n",
    "    * https://huggingface.co/docs/hub/gguf\n",
    "\n",
    "  * ì ˆì°¨\n",
    "  \n",
    "    <code>\n",
    "\n",
    "    git clone https://github.com/ggerganov/llama.cpp\n",
    "\n",
    "    cd llama.cpp\n",
    "\n",
    "    pip install -r requirements.txt\n",
    "\n",
    "    make LLAMA_CUBLAS=1\n",
    "\n",
    "    python ./convert.py ../DanteKwak/Llama-3-8B-Data-Science-Interview-QNA --outfile ../Llama-3-8B-Data-Science-Interview-QNA.gguf --vocab-type bpe\n",
    "\n",
    "    ./quantize ./Llama-3-8B-Data-Science-Interview-QNA.gguf ./Llama-3-8B-Data-Science-Interview-QNA.Q4_K_M.gguf q4_k_m\n",
    "\n",
    "    </code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Faceì— ì—…ë¡œë“œë¥¼ ìœ„í•œ API ê°ì²´ ìƒì„±\n",
    "api = HfApi()\n",
    "\n",
    "# Hugging Faceì— ëª¨ë¸ ì—…ë¡œë“œ\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"Llama-3-8B-Data-Science-Interview-QNA.Q4_K_M.gguf\",\n",
    "    path_in_repo=\"Llama-3-8B-Data-Science-Interview-QNA.Q4_K_M.gguf\",\n",
    "    repo_id=\"DanteKwak/Llama-3-8B-Data-Science-Interview-QNA-GGUF\", #ëª¨ë¸ê³µê°„ì€ ë¯¸ë¦¬ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ollamaì— ë°°í¬í•˜ê¸° ìœ„í•´ Modelfile ìƒì„±\n",
    "  \n",
    "  * gguf ì™€ ë™ì¼í•œ íŒŒì¼ìœ„ì¹˜ì— Modelfile íŒŒì¼ì„ ë§Œë“¤ì–´ ì•„ë˜ ë‚´ìš©ì„ ê¸°ì…\n",
    "  \n",
    "    ```\n",
    "    FROM Llama-3-8B-Data-Science-Interview-QNA.Q4_K_M.gguf\n",
    "\n",
    "    TEMPLATE \"\"\"{{- if .System }}\n",
    "    <s>{{ .System }}</s>\n",
    "    {{- end }}\n",
    "    <s>Human:\n",
    "    {{ .Prompt }}</s>\n",
    "    <s>Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    SYSTEM \"\"\"ìƒëŒ€ë°©ì˜ ìš”ì²­ì— ìµœëŒ€í•œ ìì„¸í•˜ê³  ì „ë¬¸ê°€ ë‹µê²Œ ë‹µí•´ì£¼ì„¸ìš”. ëª¨ë“  ì‘ë‹µì€ í•œê¸€ë¡œ í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "\n",
    "    PARAMETER temperature 0\n",
    "    PARAMETER num_predict 3000\n",
    "    PARAMETER num_ctx 4096\n",
    "    PARAMETER stop <s>\n",
    "    PARAMETER stop </s>\n",
    "    ```\n",
    "\n",
    "  * Ollama ì— ëª¨ë¸ì¶”ê°€ ë° ì‹¤í–‰\n",
    "  \n",
    "    ```\n",
    "    ollama create llama3-ds-qna -f Modelfile\n",
    "\n",
    "    ollama run llama3-ds-qna\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ollama ëª¨ë¸ì„ Langchainì— í†µí•©í•˜ì—¬ ì§ˆì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Ollama LLM ê°ì²´ ìƒì„±\n",
    "llm_ds_qna = Ollama(model=\"llama3-ds-qna\")\n",
    "\n",
    "# ì§ˆì˜ ì˜ˆì‹œ\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "chain = prompt | llm_ds_qna\n",
    "\n",
    "chain.invoke({\"question\": \"ABtestì—ì„œ ê°€ì¥ ì¤‘ìš”í•œê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AWQ(AI Weight Quantization)ë¥¼ ì´ìš©í•œ ì–‘ìí™”\n",
    "  \n",
    "  * ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì–‘ìí™”í•˜ì—¬ ëª¨ë¸ì˜ í¬ê¸°ì™€ ì—°ì‚° ì†ë„ë¥¼ ìµœì í™”í•˜ëŠ” ê¸°ìˆ \n",
    "  \n",
    "  * ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì •ë°€ë„ê°€ ë‚®ì€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¤„ì´ê³  ê³„ì‚° ì†ë„ë¥¼ ë†’ì´ëŠ” ë°©ë²•\n",
    "  * ì–‘ìí™” ê¸°ë²•:\n",
    "\t  * ê· ë“± ì–‘ìí™”(Uniform Quantization): ê°€ì¤‘ì¹˜ë¥¼ ì¼ì • ê°„ê²©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì–‘ìí™”\n",
    "\t  * ë¹„ê· ë“± ì–‘ìí™”(Non-Uniform Quantization): ë°ì´í„° ë¶„í¬ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ê°„ê²©ì„ ë‹¤ë¥´ê²Œ ì„¤ì •í•˜ì—¬ ì–‘ìí™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "quant_path = f'DanteKwak/{final_model_name}-AWQ'\n",
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "final_model_for_awq = AutoAWQForCausalLM.from_pretrained(final_model_name, **{\"low_cpu_mem_usage\": True})\n",
    "tokenizer = AutoTokenizer.from_pretrained(final_model_name, trust_remote_code=True)\n",
    "\n",
    "# ì–‘ìí™”\n",
    "final_model_for_awq.quantize(tokenizer, quant_config=quant_config)\n",
    "\n",
    "# ì–‘ìí™” ëœ ëª¨ë¸ ì €ì¥\n",
    "final_model_for_awq.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)\n",
    "\n",
    "# ëª¨ë¸ ì—…ë¡œë“œ\n",
    "api.upload_folder(\n",
    "    folder_path=quant_path,\n",
    "    repo_id=quant_path,\n",
    "    repo_type='model'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
