{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 라이브러리 설치\n",
    "# !pip install langchain\n",
    "# !pip install langchain-experimental\n",
    "# !pip install tabulate\n",
    "# !pip install llama_index\n",
    "# !pip install llama-index-experimental\n",
    "# !pip install llama-index-llms-ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import subprocess\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #부록. ollama pandas 활용하기\n",
    "* llama3:7b 한국어 파인튜닝 모델 다운로드 \n",
    "\n",
    "* ollama run taewan2002/kollama\n",
    "* 도커 환경에서 수업을 진행하는 분은 기존 도커를 down 하고 lecture11폴더의 도커를 up 해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama 실행여부 확인 (MAC 사용자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama가 실행중이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "# ollama 실행여부 확인\n",
    "try:\n",
    "    output = subprocess.check_output(['pgrep', 'ollama'])\n",
    "    print(f\"Ollama 실행중: PID({output.decode('utf-8').strip()})\")\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"Ollama가 실행중이 아닙니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama 실행여부 확인 (윈도우 사용자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    output = subprocess.check_output(['tasklist', '/FI', 'IMAGENAME eq ollama.exe'])\n",
    "    if b'ollama.exe' in output:\n",
    "        print(\"Ollama 실행중\")\n",
    "    else:\n",
    "        print(\"Ollama가 실행중이 아닙니다.\")\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"Ollama가 실행중이 아닙니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollma 시작하기 (공통))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['ollama', 'serve']>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/17 18:49:41 routes.go:1008: INFO server config env=\"map[OLLAMA_DEBUG:false OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]\"\n",
      "time=2024-05-17T18:49:41.509+09:00 level=INFO source=images.go:704 msg=\"total blobs: 14\"\n",
      "time=2024-05-17T18:49:41.512+09:00 level=INFO source=images.go:711 msg=\"total unused blobs removed: 0\"\n",
      "time=2024-05-17T18:49:41.512+09:00 level=INFO source=routes.go:1054 msg=\"Listening on 127.0.0.1:11434 (version 0.1.38)\"\n",
      "time=2024-05-17T18:49:41.514+09:00 level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/var/folders/5y/vwfpdbc96lb5kxd7pmfd9rl00000gn/T/ollama2479225659/runners\n",
      "time=2024-05-17T18:49:41.539+09:00 level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [metal]\"\n",
      "time=2024-05-17T18:49:41.592+09:00 level=INFO source=types.go:71 msg=\"inference compute\" id=0 library=metal compute=\"\" driver=0.0 name=\"\" total=\"21.3 GiB\" available=\"21.3 GiB\"\n"
     ]
    }
   ],
   "source": [
    "subprocess.Popen(['ollama', 'serve'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama 종료하기 (MAC 사용자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='pgrep ollama | xargs kill -9', returncode=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subprocess.run(\"pgrep ollama | xargs kill -9\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama 종료하기 (윈도우 사용자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\"taskkill /F /IM ollama.exe\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama 접속확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/05/17 - 18:50:35 | 200 |    1.647542ms |       127.0.0.1 | GET      \"/\"\n",
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "# Ollama 연결확인\n",
    "BASE_URL = 'http://localhost:11434'\n",
    "response = requests.get(BASE_URL) \n",
    "print(response.content.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain 클라이언트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = Ollama(model=\"EEVE-Korean-10.8B:latest\", base_url=BASE_URL)\n",
    "llm = Ollama(model=\"llama3:latest\", base_url=BASE_URL)\n",
    "# llm = Ollama(model=\"mistral:latest\", base_url=BASE_URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('../lecture09/datasets/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType\n",
    "from langchain_experimental.agents import create_pandas_dataframe_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df_agent = create_pandas_dataframe_agent(\n",
    "    llm,\n",
    "    movies_df,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/05/17 - 19:39:51 | 200 |   1.19524275s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:39:54 | 200 |  3.017601666s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:39:57 | 200 |  2.902868875s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:40:00 | 200 |  3.651134625s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:40:05 | 200 |   4.66035225s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:40:09 | 200 |  3.550822417s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:40:12 | 200 |     3.052642s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:40:14 | 200 |  2.788350625s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:40:18 | 200 |  3.636145667s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:40:21 | 200 |  3.094717625s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:40:26 | 200 |  5.119388333s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:40:33 | 200 |  6.982511333s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:40:38 | 200 |  4.894113083s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:40:43 | 200 |  4.521061916s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:40:47 | 200 |  4.574868208s |       127.0.0.1 | POST     \"/api/generate\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/05/17 - 19:45:14 | 200 |  3.392984917s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:45:16 | 200 |  1.713403625s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:45:20 | 200 |  3.656848042s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:45:22 | 200 |  2.613548667s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:45:43 | 200 |  2.277759875s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:45:45 | 200 |  1.871921625s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:45:47 | 200 |  1.706977125s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:45:50 | 200 |  2.679646708s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:45:53 | 200 |  3.583316625s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:45:57 | 200 |   3.32560225s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:46:02 | 200 |  5.247944542s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:46:05 | 200 |  3.144973167s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:46:11 | 200 |     5.670942s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:46:19 | 200 |  8.488700875s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:46:25 | 200 |   6.22282775s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:46:50 | 200 |    1.6691465s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:46:51 | 200 |   963.14925ms |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:46:53 | 200 |  1.351635458s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:46:54 | 200 |  1.757236583s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:46:56 | 200 |  1.453122291s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:46:57 | 200 |  1.371902959s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:46:59 | 200 |  1.388169291s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:47:00 | 200 |  1.458485875s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:48:20 | 200 |  1.778679041s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:48:22 | 200 |  1.888264958s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:48:25 | 200 |  2.690433334s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:48:28 | 200 |  2.715660042s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:48:31 | 200 |  2.930994084s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:48:34 | 200 |  3.640620041s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:48:39 | 200 |     4.346009s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:48:45 | 200 |   5.99606525s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:48:50 | 200 |  5.785156542s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:48:57 | 200 |  6.839604708s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:53:55 | 200 |  3.745435916s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:53:57 | 200 |  1.827906959s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:53:59 | 200 |  2.722315375s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:54:02 | 200 |   2.53431675s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:54:04 | 200 |   2.43691725s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:54:07 | 200 |    2.8114715s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:54:10 | 200 |    2.6780485s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:54:12 | 200 |  2.530931375s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:54:15 | 200 |  2.855220375s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:54:18 | 200 |  2.686617417s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:54:23 | 200 |  5.311980458s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:55:02 | 200 |    3.4375395s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:55:05 | 200 |  3.045911084s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:55:08 | 200 |  3.084030125s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:55:11 | 200 |  2.620108708s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:55:13 | 200 |  2.475609375s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:55:15 | 200 |  2.330913834s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:55:18 | 200 |  2.281588584s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:55:20 | 200 |  2.215569042s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:55:23 | 200 |  3.310003375s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:55:26 | 200 |  2.367932333s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:55:28 | 200 |  2.439211459s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:55:40 | 200 |  11.78691475s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/05/17 - 19:55:50 | 200 | 10.637884833s |       127.0.0.1 | POST     \"/api/generate\"\n"
     ]
    }
   ],
   "source": [
    "pandas_df_agent.invoke(\"how many Action genre movies\")['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.experimental.query_engine.pandas import PandasQueryEngine\n",
    "from llama_index.llms.ollama import Ollama\n",
    "# llm = Ollama(model=\"llama3:latest\", request_timeout=60.0)\n",
    "llm = Ollama(model=\"EEVE-Korean-10.8B:latest\", request_timeout=60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-05-17T20:04:45.202+09:00 level=INFO source=memory.go:133 msg=\"offload to gpu\" layers.requested=-1 layers.real=49 memory.available=\"21.3 GiB\" memory.required.full=\"8.7 GiB\" memory.required.partial=\"8.7 GiB\" memory.required.kv=\"731.2 MiB\" memory.weights.total=\"7.0 GiB\" memory.weights.repeating=\"6.9 GiB\" memory.weights.nonrepeating=\"131.3 MiB\" memory.graph.full=\"283.4 MiB\" memory.graph.partial=\"283.4 MiB\"\n",
      "time=2024-05-17T20:04:45.203+09:00 level=INFO source=memory.go:133 msg=\"offload to gpu\" layers.requested=-1 layers.real=49 memory.available=\"21.3 GiB\" memory.required.full=\"8.7 GiB\" memory.required.partial=\"8.7 GiB\" memory.required.kv=\"731.2 MiB\" memory.weights.total=\"7.0 GiB\" memory.weights.repeating=\"6.9 GiB\" memory.weights.nonrepeating=\"131.3 MiB\" memory.graph.full=\"283.4 MiB\" memory.graph.partial=\"283.4 MiB\"\n",
      "time=2024-05-17T20:04:45.204+09:00 level=INFO source=server.go:320 msg=\"starting llama server\" cmd=\"/var/folders/5y/vwfpdbc96lb5kxd7pmfd9rl00000gn/T/ollama2479225659/runners/metal/ollama_llama_server --model /Users/dante/.ollama/models/blobs/sha256-b9e3d1ad5e8aa6db09610d4051820f06a5257b7d7f0b06c00630e376abcfa4c1 --ctx-size 3900 --batch-size 512 --embedding --log-disable --n-gpu-layers 49 --parallel 1 --port 60037\"\n",
      "time=2024-05-17T20:04:45.206+09:00 level=INFO source=sched.go:338 msg=\"loaded runners\" count=1\n",
      "time=2024-05-17T20:04:45.206+09:00 level=INFO source=server.go:504 msg=\"waiting for llama runner to start responding\"\n",
      "time=2024-05-17T20:04:45.206+09:00 level=INFO source=server.go:540 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 435 tensors from /Users/dante/.ollama/models/blobs/sha256-b9e3d1ad5e8aa6db09610d4051820f06a5257b7d7f0b06c00630e376abcfa4c1 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 48\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,40960]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,40960]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,40960]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   97 tensors\n",
      "llama_model_loader: - type q5_K:  289 tensors\n",
      "llama_model_loader: - type q6_K:   49 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 261/40960 vs 260/40960 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 40960\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 48\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 34B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 10.80 B\n",
      "llm_load_print_meta: model size       = 7.13 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.44 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  7188.03 MiB, ( 7188.09 / 21845.34)\n",
      "llm_load_tensors: offloading 48 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 49/49 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   110.00 MiB\n",
      "llm_load_tensors:      Metal buffer size =  7188.03 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Max\n",
      "ggml_metal_init: picking default device: Apple M2 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   732.00 MiB, ( 7921.91 / 21845.34)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   732.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  732.00 MiB, K (f16):  366.00 MiB, V (f16):  366.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.17 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   283.64 MiB, ( 8205.55 / 21845.34)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   283.63 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    15.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1542\n",
      "llama_new_context_with_model: graph splits = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO [main] build info | build=2770 commit=\"952d03d\" tid=\"0x2076cbac0\" timestamp=1715943885\n",
      "INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info=\"AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"0x2076cbac0\" timestamp=1715943885 total_threads=12\n",
      "INFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"11\" port=\"60037\" tid=\"0x2076cbac0\" timestamp=1715943885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-05-17T20:04:45.457+09:00 level=INFO source=server.go:540 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO [main] model loaded | tid=\"0x2076cbac0\" timestamp=1715943896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-05-17T20:04:57.022+09:00 level=INFO source=server.go:545 msg=\"llama runner started in 11.82 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/05/17 - 20:05:03 | 200 | 18.852050291s |       127.0.0.1 | POST     \"/api/generate\"\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"한국의 대통령은 누구인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 한국의 현직 대통령은 윤석열입니다. 그는 2022년 5월 10일에 취임한 대한민국의 제20대 대통령입니다. 윤 대통령은 전직 검찰총장이며, 보수 성향의 국민의힘 소속입니다. 그의 임기는 2027년 5월까지 예정되어 있습니다.\n",
      "\n",
      "윤 대통령은 법무부 장관과 서울 중앙지검장을 역임하며 부패와 싸우고 강력한 리더십을 발휘한 것으로 알려져 있습니다. 그는 경제 개혁, 안보 강화 및 북한과의 외교 노력에 중점을 둔 의제로 취임했습니다.\n",
      "\n",
      "도움이 되셨길 바랍니다! 궁금한 점이나 더 알고 싶은 사항이 있다면 언제든지 질문해 주세요.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/05/17 - 20:06:35 | 200 |  4.340917792s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "안녕하세요! 궁금한 점을 도와드리겠습니다. 한국의 바로 전직 대통령은 문재인입니다. 그는 2017년 5월부터 2022년 5월까지 재임하셨으며, 이후 윤석열 대통령이 취임하였습니다. 다른 도움이 필요하신가요?\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"한국의 바로 직전 대통령은 누구인가요?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Pandas Instructions:\n",
      "```\n",
      "df.shape\n",
      "```\n",
      "> Pandas Output: (9742, 3)\n",
      "(9742, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../lecture09/datasets/movies.csv')\n",
    "\n",
    "query_engine = PandasQueryEngine(df=df, verbose=True)\n",
    "\n",
    "response = query_engine.query(\"행과 열의 크기를 출력해줘\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Pandas Instructions:\n",
      "```\n",
      "df[df['genres'].str.contains('Action')].shape[0]\n",
      "```\n",
      "> Pandas Output: 1828\n",
      "1828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/05/18 - 07:24:54 | 404 |     528.042µs |       127.0.0.1 | POST     \"/api/show\"\n",
      "[GIN] 2024/05/18 - 07:24:54 | 404 |     257.542µs |       127.0.0.1 | POST     \"/api/show\"\n",
      "[GIN] 2024/05/18 - 07:24:54 | 404 |     348.667µs |       127.0.0.1 | POST     \"/api/show\"\n"
     ]
    }
   ],
   "source": [
    "request = \"액션 영화는 몇개야\"\n",
    "response = query_engine.query(request)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
